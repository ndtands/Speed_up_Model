INFO 2022-06-06 15:39:13,013 [4142902042.py:<cell line: 8>:21]
Time for inference: 0.012606620788574219

INFO 2022-06-06 15:42:14,594 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312, %, index: 151

INFO 2022-06-06 15:42:14,597 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035, %, index: 237

INFO 2022-06-06 15:42:14,599 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775, %, index: 171

INFO 2022-06-06 15:42:14,600 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583, %, index: 158

INFO 2022-06-06 15:42:14,601 [inference.py:<module>:29]
Time for inference: 0.0424041748046875

INFO 2022-06-06 15:42:45,967 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312, %, index: 151

INFO 2022-06-06 15:42:45,970 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035, %, index: 237

INFO 2022-06-06 15:42:45,971 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775, %, index: 171

INFO 2022-06-06 15:42:45,972 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583, %, index: 158

INFO 2022-06-06 15:42:45,973 [inference.py:<module>:29]
Time for inference:  0.041872 s

INFO 2022-06-06 15:43:46,720 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312, %, index: 151

INFO 2022-06-06 15:43:46,723 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035, %, index: 237

INFO 2022-06-06 15:43:46,724 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775, %, index: 171

INFO 2022-06-06 15:43:46,725 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583, %, index: 158

INFO 2022-06-06 15:43:46,727 [inference.py:<module>:29]
Time for inference:  0.048118 s

INFO 2022-06-06 15:45:15,649 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312, %, index: 151

INFO 2022-06-06 15:45:15,652 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035, %, index: 237

INFO 2022-06-06 15:45:15,654 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775, %, index: 171

INFO 2022-06-06 15:45:15,655 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583, %, index: 158

INFO 2022-06-06 15:45:15,656 [inference.py:<module>:29]
Time for inference:  0.046484 s

INFO 2022-06-06 15:54:13,267 [test.py:test_speed:19]

                    Batch size: 2 
                    Time inferene with 100 loop is 1.2635 s
                    >>>> About: 0.012635 s per 1 loop
                

INFO 2022-06-06 15:54:42,379 [test.py:test_speed:19]

        Batch size: 2 
        Time inferene with 100 loop is 1.2546 s
        >>>> About: 0.012546 s per 1 loop
    

INFO 2022-06-06 15:54:52,569 [test.py:test_speed:19]

    Batch size: 2 
    Time inferene with 100 loop is 1.2807 s
    >>>> About: 0.012807 s per 1 loop
    

INFO 2022-06-06 15:58:59,563 [test.py:test_speed:20]

    Batch size: 4 
    Time inferene with 100 loop is 2.1387 s
    >>>> About: 0.021387 s per 1 loop
    

INFO 2022-06-06 15:59:22,482 [test.py:test_speed:20]

    Batch size: 8 
    Time inferene with 20 loop is 0.7326 s
    >>>> About: 0.036629 s per 1 loop
    

INFO 2022-06-06 16:06:53,642 [utils.py:postprocess:37]
class: cup , confidence: 58.57439041137695, %, index: 968

INFO 2022-06-06 16:06:53,645 [utils.py:postprocess:37]
class: espresso , confidence: 38.98651885986328, %, index: 967

INFO 2022-06-06 16:06:53,646 [utils.py:postprocess:37]
class: coffee mug , confidence: 2.20611572265625, %, index: 504

INFO 2022-06-06 16:06:53,647 [inference.py:<module>:29]
Time for inference:  0.043351 s

INFO 2022-06-06 16:08:09,310 [test.py:test_speed:20]

    Batch size: 4 
    Time inferene with 100 loop is 2.1143 s
    >>>> About: 0.021143 s per 1 loop
    

INFO 2022-06-06 16:34:33,055 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312, %, index: 151

INFO 2022-06-06 16:34:33,060 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035, %, index: 237

INFO 2022-06-06 16:34:33,061 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775, %, index: 171

INFO 2022-06-06 16:34:33,063 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583, %, index: 158

INFO 2022-06-06 16:34:33,064 [inference.py:<module>:29]
Time for inference:  0.145030 s

INFO 2022-06-06 16:59:05,204 [2565630689.py:convert:19]
===> build onnx ...

INFO 2022-06-06 16:59:07,190 [2565630689.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-06 17:00:09,923 [2565630689.py:convert:19]
===> build onnx ...

INFO 2022-06-06 17:00:11,783 [2565630689.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-06 17:01:17,110 [1200333086.py:load:37]
Load onnx done with {self.device}

INFO 2022-06-06 17:01:31,570 [2111965102.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:02:53,201 [2111965102.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:05:17,805 [2111965102.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:05:37,862 [3254488601.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:05:55,915 [4181733994.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:05:56,504 [4181733994.py:postprocess_onnx:75]
class: Chihuahua , confidence: 0.8124645352363586, %, index: 151

INFO 2022-06-06 17:05:56,506 [4181733994.py:inference:49]
Time for inference:  0.543981 s

INFO 2022-06-06 17:06:19,349 [21089939.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:06:19,974 [21089939.py:postprocess_onnx:75]
class: Chihuahua , confidence: 81.24644470214844, %, index: 151

INFO 2022-06-06 17:06:19,976 [21089939.py:postprocess_onnx:75]
class: miniature pinscher , confidence: 15.077646255493164, %, index: 237

INFO 2022-06-06 17:06:19,978 [21089939.py:postprocess_onnx:75]
class: Italian greyhound , confidence: 1.657345175743103, %, index: 171

INFO 2022-06-06 17:06:19,980 [21089939.py:postprocess_onnx:75]
class: toy terrier , confidence: 1.0198869705200195, %, index: 158

INFO 2022-06-06 17:06:19,982 [21089939.py:inference:49]
Time for inference:  0.551140 s

INFO 2022-06-06 17:07:05,492 [3727547070.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:07:06,109 [3727547070.py:postprocess_onnx:75]
class: Chihuahua , confidence:  81.2464, %, index: 151

INFO 2022-06-06 17:07:06,113 [3727547070.py:postprocess_onnx:75]
class: miniature pinscher , confidence:  15.0776, %, index: 237

INFO 2022-06-06 17:07:06,116 [3727547070.py:postprocess_onnx:75]
class: Italian greyhound , confidence:  1.6573, %, index: 171

INFO 2022-06-06 17:07:06,119 [3727547070.py:postprocess_onnx:75]
class: toy terrier , confidence:  1.0199, %, index: 158

INFO 2022-06-06 17:07:06,123 [3727547070.py:inference:49]
Time for inference:  0.584333 s

INFO 2022-06-06 17:14:55,385 [__init__.py:convert:19]
===> build onnx ...

INFO 2022-06-06 17:14:57,092 [__init__.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-06 17:20:02,502 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:20:39,869 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:22:24,344 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:22:24,839 [__init__.py:postprocess_onnx:76]
class: Chihuahua , confidence:  81.2464 %, index: 151

INFO 2022-06-06 17:22:24,844 [__init__.py:postprocess_onnx:76]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 17:22:24,845 [__init__.py:postprocess_onnx:76]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 17:22:24,846 [__init__.py:postprocess_onnx:76]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 17:22:24,847 [__init__.py:inference:49]
Time for inference:  0.497503 s

INFO 2022-06-06 17:22:57,886 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.24636840820312 %, index: 151

INFO 2022-06-06 17:22:57,889 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.077691078186035 %, index: 237

INFO 2022-06-06 17:22:57,890 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573545932769775 %, index: 171

INFO 2022-06-06 17:22:57,891 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0198899507522583 %, index: 158

INFO 2022-06-06 17:22:57,892 [inference.py:<module>:28]
Time for inference:  0.055912 s

INFO 2022-06-06 17:24:56,553 [3000866572.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:25:00,251 [3000866572.py:postprocess_onnx:75]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-06 17:25:00,253 [3000866572.py:postprocess_onnx:75]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 17:25:00,307 [3000866572.py:postprocess_onnx:75]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 17:25:00,309 [3000866572.py:postprocess_onnx:75]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 17:25:00,311 [3000866572.py:inference:49]
Time for inference:  0.605105 s

INFO 2022-06-06 17:25:04,774 [3000866572.py:postprocess_onnx:75]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-06 17:25:04,777 [3000866572.py:postprocess_onnx:75]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 17:25:04,779 [3000866572.py:postprocess_onnx:75]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 17:25:04,782 [3000866572.py:postprocess_onnx:75]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 17:25:04,785 [3000866572.py:inference:49]
Time for inference:  0.027165 s

INFO 2022-06-06 17:25:08,203 [3000866572.py:postprocess_onnx:75]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-06 17:25:08,205 [3000866572.py:postprocess_onnx:75]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 17:25:08,209 [3000866572.py:postprocess_onnx:75]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 17:25:08,211 [3000866572.py:postprocess_onnx:75]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 17:25:08,213 [3000866572.py:inference:49]
Time for inference:  0.023844 s

INFO 2022-06-06 17:25:12,577 [3000866572.py:postprocess_onnx:75]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-06 17:25:12,579 [3000866572.py:postprocess_onnx:75]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 17:25:12,582 [3000866572.py:postprocess_onnx:75]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 17:25:12,585 [3000866572.py:postprocess_onnx:75]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 17:25:12,588 [3000866572.py:inference:49]
Time for inference:  0.024749 s

INFO 2022-06-06 17:29:03,385 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:29:32,085 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 17:29:34,435 [test.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.3431 s
    >>>> About: 0.023431 s per 1 loop
    

INFO 2022-06-06 21:16:21,165 [convert.py:<module>:15]
Build tensorrt engine...

INFO 2022-06-06 21:16:22,684 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-06 21:16:22,685 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (32, 3, 224, 224), (32, 3, 224, 224))}

INFO 2022-06-06 21:16:22,686 [__init__.py:build_engine:95]
✅ Creating Tensorrt Engine...

INFO 2022-06-06 21:19:44,369 [convert.py:<module>:15]
Build tensorrt engine...

INFO 2022-06-06 21:19:45,634 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-06 21:19:45,635 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (2, 3, 224, 224), (2, 3, 224, 224))}

INFO 2022-06-06 21:19:45,641 [__init__.py:build_engine:95]
Creating Tensorrt Engine...

INFO 2022-06-06 21:24:53,191 [convert.py:<module>:14]
Build tensorrt engine...

INFO 2022-06-06 21:24:54,457 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-06 21:24:54,458 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (1, 3, 224, 224), (1, 3, 224, 224))}

INFO 2022-06-06 21:24:54,464 [__init__.py:build_engine:95]
Creating Tensorrt Engine...

INFO 2022-06-06 21:26:23,062 [convert.py:<module>:14]
Build tensorrt engine...

INFO 2022-06-06 21:26:24,369 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-06 21:26:24,370 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (32, 3, 224, 224), (32, 3, 224, 224))}

INFO 2022-06-06 21:26:24,376 [__init__.py:build_engine:95]
Creating Tensorrt Engine...

INFO 2022-06-06 21:28:52,570 [__init__.py:build_engine:100]
===> Serialized Engine Saved at: /home/tari/Desktop/STUDY/Optimization/model/model.trt

INFO 2022-06-06 21:44:20,294 [test.py:<module>:14]
✅ Load tensorrt engine done

INFO 2022-06-06 21:55:16,113 [test.py:<module>:19]
✅ Load tensorrt engine done

INFO 2022-06-06 21:55:16,985 [test.py:<module>:26]

    Batch size: 4 
    Time inferene with 100 loop is 0.8646 s
    >>>> About: 0.008646 s per 1 loop
    

INFO 2022-06-06 21:55:31,611 [test.py:<module>:19]
✅ Load tensorrt engine done

INFO 2022-06-06 21:55:33,252 [test.py:<module>:26]

    Batch size: 8 
    Time inferene with 100 loop is 1.6288 s
    >>>> About: 0.016288 s per 1 loop
    

INFO 2022-06-06 21:55:47,943 [test.py:<module>:19]
✅ Load tensorrt engine done

INFO 2022-06-06 21:55:53,621 [test.py:<module>:26]

    Batch size: 32 
    Time inferene with 100 loop is 5.6419 s
    >>>> About: 0.056419 s per 1 loop
    

INFO 2022-06-06 22:00:56,918 [inference.py:<module>:17]
✅ Load tensorrt engine done

INFO 2022-06-06 22:01:14,705 [inference.py:<module>:17]
✅ Load tensorrt engine done

INFO 2022-06-06 22:02:01,623 [inference.py:<module>:17]
✅ Load tensorrt engine done

INFO 2022-06-06 22:02:01,639 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-06 22:02:01,641 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-06 22:02:01,642 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-06 22:02:01,643 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-06 22:02:01,644 [inference.py:<module>:24]
Time for inference:  0.014279 s

INFO 2022-06-06 22:58:46,095 [inference.py:<module>:19]
>>>>> Pretrain model <<<<<

INFO 2022-06-06 22:58:49,384 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-06 22:58:49,385 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0777 %, index: 237

INFO 2022-06-06 22:58:49,387 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6574 %, index: 171

INFO 2022-06-06 22:58:49,388 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-06 22:58:49,389 [inference.py:<module>:30]
Time for inference:  0.083843 s

INFO 2022-06-06 22:59:45,343 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-06 22:59:48,866 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 22:59:49,612 [__init__.py:postprocess_onnx:76]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-06 22:59:49,613 [__init__.py:postprocess_onnx:76]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-06 22:59:49,614 [__init__.py:postprocess_onnx:76]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 22:59:49,615 [__init__.py:postprocess_onnx:76]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 22:59:49,615 [__init__.py:inference:49]
Time for inference:  0.744216 s

INFO 2022-06-06 23:00:01,347 [inference.py:<module>:17]
>>>>> TensorRt engine <<<<<

INFO 2022-06-06 23:00:01,350 [inference.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-06 23:00:01,363 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-06 23:00:01,365 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-06 23:00:01,366 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-06 23:00:01,367 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-06 23:00:01,368 [inference.py:<module>:24]
Time for inference:  0.013342 s

INFO 2022-06-06 23:02:20,171 [inference.py:<module>:19]
>>>>> Pretrain model <<<<<

INFO 2022-06-06 23:02:22,502 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-06 23:02:22,503 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0777 %, index: 237

INFO 2022-06-06 23:02:22,504 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6574 %, index: 171

INFO 2022-06-06 23:02:22,505 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-06 23:02:22,506 [inference.py:<module>:30]
Time for inference:  0.045924 s

INFO 2022-06-06 23:02:30,975 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-06 23:02:34,063 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-06 23:02:34,571 [__init__.py:postprocess_onnx:78]
class: Chihuahua , confidence:  81.2464 %, index: 151

INFO 2022-06-06 23:02:34,572 [__init__.py:postprocess_onnx:78]
class: miniature pinscher , confidence:  15.0777 %, index: 237

INFO 2022-06-06 23:02:34,573 [__init__.py:postprocess_onnx:78]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-06 23:02:34,574 [__init__.py:postprocess_onnx:78]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-06 23:02:34,575 [__init__.py:inference:51]
Time for inference:  0.018605 s

INFO 2022-06-06 23:02:41,727 [inference.py:<module>:17]
>>>>> TensorRt engine <<<<<

INFO 2022-06-06 23:02:41,731 [inference.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-06 23:02:41,744 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-06 23:02:41,746 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-06 23:02:41,747 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-06 23:02:41,748 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-06 23:02:41,749 [inference.py:<module>:24]
Time for inference:  0.013857 s

INFO 2022-06-07 14:48:04,250 [test.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 14:48:10,014 [test.py:<module>:25]

    Batch size: 32 
    Time inferene with 100 loop is 5.6301 s
    >>>> About: 0.056301 s per 1 loop
    

INFO 2022-06-07 15:47:25,297 [inference.py:<module>:19]
>>>>> Pretrain model <<<<<

INFO 2022-06-07 15:47:28,919 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 15:47:28,921 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0777 %, index: 237

INFO 2022-06-07 15:47:28,922 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6574 %, index: 171

INFO 2022-06-07 15:47:28,923 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 15:47:28,924 [inference.py:<module>:30]
Time for inference:  0.111649 s

INFO 2022-06-07 15:47:43,454 [test.py:test_speed:20]

    Batch size: 4 
    Time inferene with 100 loop is 2.1242 s
    >>>> About: 0.021242 s per 1 loop
    

INFO 2022-06-07 15:48:02,058 [test.py:test_speed:20]

    Batch size: 4 
    Time inferene with 100 loop is 2.1162 s
    >>>> About: 0.021162 s per 1 loop
    

INFO 2022-06-07 15:50:09,128 [__init__.py:convert:19]
===> build onnx ...

INFO 2022-06-07 15:50:10,734 [__init__.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-07 15:51:46,130 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 15:51:49,984 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 15:51:50,779 [__init__.py:postprocess_onnx:78]
class: Chihuahua , confidence:  81.2464 %, index: 151

INFO 2022-06-07 15:51:50,781 [__init__.py:postprocess_onnx:78]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-07 15:51:50,782 [__init__.py:postprocess_onnx:78]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 15:51:50,783 [__init__.py:postprocess_onnx:78]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 15:51:50,783 [__init__.py:inference:51]
Time for inference:  0.019741 s

INFO 2022-06-07 15:52:18,974 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 15:52:21,307 [test.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.3260 s
    >>>> About: 0.023260 s per 1 loop
    

INFO 2022-06-07 15:52:35,815 [convert.py:<module>:14]
Build tensorrt engine...

INFO 2022-06-07 15:52:37,347 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-07 15:52:37,348 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (32, 3, 224, 224), (32, 3, 224, 224))}

INFO 2022-06-07 15:52:37,350 [__init__.py:build_engine:95]
Creating Tensorrt Engine...

INFO 2022-06-07 15:55:06,457 [__init__.py:build_engine:100]
===> Serialized Engine Saved at: /home/tari/Desktop/STUDY/Optimization/model_repository/model_trt/1/model.trt

INFO 2022-06-07 15:55:33,976 [inference.py:<module>:17]
>>>>> TensorRt engine <<<<<

INFO 2022-06-07 15:55:33,979 [inference.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 15:55:33,994 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-07 15:55:33,995 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-07 15:55:33,996 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-07 15:55:33,997 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-07 15:55:33,999 [inference.py:<module>:24]
Time for inference:  0.015842 s

INFO 2022-06-07 15:55:49,283 [test.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 15:55:54,983 [test.py:<module>:25]

    Batch size: 32 
    Time inferene with 100 loop is 5.6553 s
    >>>> About: 0.056553 s per 1 loop
    

INFO 2022-06-07 16:35:18,753 [test.py:<module>:18]


INFO 2022-06-07 16:35:18,765 [test.py:<module>:25]
✅ Load triton client done

INFO 2022-06-07 16:35:20,921 [test.py:<module>:30]

    Batch size: 4 
    Time inferene with 100 loop is 2.1548 s
    >>>> About: 0.021548 s per 1 loop
    

INFO 2022-06-07 16:38:55,660 [test.py:<module>:25]
✅ Load triton client done

INFO 2022-06-07 16:38:57,585 [test.py:<module>:30]

    Batch size: 4 
    Time inferene with 100 loop is 1.9204 s
    >>>> About: 0.019204 s per 1 loop
    

INFO 2022-06-07 16:39:05,760 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:39:13,614 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:39:13,618 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:39:13,780 [inference.py:<module>:28]
Time for inference:  0.157799 s

INFO 2022-06-07 16:39:36,962 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:39:36,964 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:39:36,996 [inference.py:<module>:29]
Time for inference:  0.027035 s

INFO 2022-06-07 16:40:12,795 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:40:12,798 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:40:12,832 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:40:12,833 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:40:12,835 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:40:12,836 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:40:12,837 [inference.py:<module>:30]
Time for inference:  0.034474 s

INFO 2022-06-07 16:40:35,017 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:40:35,020 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:40:35,048 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:40:35,050 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:40:35,051 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:40:35,052 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:40:35,054 [inference.py:<module>:30]
Time for inference:  0.029452 s

INFO 2022-06-07 16:41:28,055 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:41:28,057 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:41:28,100 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:41:28,102 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:41:28,103 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:41:28,104 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:41:28,104 [inference.py:<module>:33]
Time for inference:  0.015692 s

INFO 2022-06-07 16:41:42,244 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:41:42,247 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:41:42,275 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:41:42,277 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:41:42,278 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:41:42,280 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:41:42,281 [inference.py:<module>:32]
Time for inference:  0.017038 s

INFO 2022-06-07 16:42:26,989 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:42:26,992 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:42:27,035 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:42:27,037 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:42:27,039 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:42:27,041 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:42:27,042 [inference.py:<module>:32]
Time for inference:  0.019913 s

INFO 2022-06-07 16:44:42,535 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:44:42,537 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:44:42,579 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:44:42,580 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:44:42,581 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:44:42,582 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:44:55,323 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:44:55,326 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:44:55,356 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:44:55,358 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:44:55,359 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:44:55,360 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:44:55,362 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:44:55,364 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:44:55,365 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:44:55,367 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:44:55,368 [inference.py:<module>:33]
Time for inference:  0.022664 s

INFO 2022-06-07 16:45:02,932 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 16:45:02,935 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 16:45:02,962 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 16:45:02,964 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 16:45:02,965 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 16:45:02,966 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 16:45:02,967 [inference.py:<module>:32]
Time for inference:  0.016931 s

INFO 2022-06-07 16:53:02,853 [__init__.py:convert:19]
===> build onnx ...

INFO 2022-06-07 16:53:04,457 [__init__.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-07 16:53:12,538 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 16:53:14,865 [test.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.3198 s
    >>>> About: 0.023198 s per 1 loop
    

INFO 2022-06-07 16:53:27,358 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 16:53:30,476 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 16:53:30,988 [__init__.py:postprocess_onnx:78]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-07 16:53:30,990 [__init__.py:postprocess_onnx:78]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-07 16:53:30,991 [__init__.py:postprocess_onnx:78]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 16:53:30,992 [__init__.py:postprocess_onnx:78]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 16:53:30,993 [__init__.py:inference:51]
Time for inference:  0.019308 s

INFO 2022-06-07 16:53:59,286 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 16:54:01,642 [test.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.3481 s
    >>>> About: 0.023481 s per 1 loop
    

INFO 2022-06-07 16:54:21,386 [__init__.py:convert:19]
===> build onnx ...

INFO 2022-06-07 16:54:23,050 [__init__.py:convert:30]
✅ Convert model pytorch to onnx complete

INFO 2022-06-07 16:54:45,189 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 16:54:47,533 [test.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.3383 s
    >>>> About: 0.023383 s per 1 loop
    

INFO 2022-06-07 16:54:54,536 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 16:54:57,682 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 16:54:58,179 [__init__.py:postprocess_onnx:78]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-07 16:54:58,180 [__init__.py:postprocess_onnx:78]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-07 16:54:58,181 [__init__.py:postprocess_onnx:78]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 16:54:58,182 [__init__.py:postprocess_onnx:78]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 16:54:58,182 [__init__.py:inference:51]
Time for inference:  0.018445 s

INFO 2022-06-07 16:56:02,675 [convert.py:<module>:14]
Build tensorrt engine...

INFO 2022-06-07 16:56:03,976 [__init__.py:build_engine:71]
✅ Completed parsing ONNX file

INFO 2022-06-07 16:56:03,977 [__init__.py:build_engine:77]
===> using dynamic shapes: {'input': ((1, 3, 224, 224), (32, 3, 224, 224), (32, 3, 224, 224))}

INFO 2022-06-07 16:56:03,979 [__init__.py:build_engine:95]
Creating Tensorrt Engine...

INFO 2022-06-07 16:58:32,600 [__init__.py:build_engine:100]
===> Serialized Engine Saved at: /home/tari/Desktop/STUDY/Optimization/model/model_trt/model.plan

INFO 2022-06-07 17:01:11,777 [test.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 17:01:17,482 [test.py:<module>:25]

    Batch size: 32 
    Time inferene with 100 loop is 5.6631 s
    >>>> About: 0.056631 s per 1 loop
    

INFO 2022-06-07 17:01:24,644 [inference.py:<module>:17]
>>>>> TensorRt engine <<<<<

INFO 2022-06-07 17:01:24,647 [inference.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 17:01:24,659 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-07 17:01:24,661 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-07 17:01:24,662 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-07 17:01:24,663 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-07 17:01:24,664 [inference.py:<module>:25]
Time for inference:  0.013513 s

INFO 2022-06-07 17:05:03,382 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 17:05:03,385 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 17:05:04,006 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 17:05:04,007 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 17:05:04,008 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 17:05:04,009 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 17:05:04,010 [inference.py:<module>:32]
Time for inference:  0.015661 s

INFO 2022-06-07 17:05:20,411 [test.py:<module>:25]
✅ Load triton client done

INFO 2022-06-07 17:05:34,239 [test.py:<module>:30]

    Batch size: 32 
    Time inferene with 100 loop is 13.8244 s
    >>>> About: 0.138244 s per 1 loop
    

INFO 2022-06-07 17:05:54,457 [test.py:<module>:25]
✅ Load triton client done

INFO 2022-06-07 17:05:56,634 [test.py:<module>:30]

    Batch size: 4 
    Time inferene with 100 loop is 2.1721 s
    >>>> About: 0.021721 s per 1 loop
    

INFO 2022-06-07 17:22:14,340 [inference.py:<module>:19]
>>>>> Pretrain model <<<<<

INFO 2022-06-07 17:22:16,771 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 17:22:16,772 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0777 %, index: 237

INFO 2022-06-07 17:22:16,773 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6574 %, index: 171

INFO 2022-06-07 17:22:16,774 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 17:22:16,775 [inference.py:<module>:30]
Time for inference:  0.044241 s

INFO 2022-06-07 17:22:23,608 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 17:22:27,018 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 17:22:27,581 [__init__.py:postprocess_onnx:78]
class: Chihuahua , confidence:  81.2464 %, index: 151

INFO 2022-06-07 17:22:27,582 [__init__.py:postprocess_onnx:78]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-07 17:22:27,583 [__init__.py:postprocess_onnx:78]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 17:22:27,584 [__init__.py:postprocess_onnx:78]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 17:22:27,586 [__init__.py:inference:51]
Time for inference:  0.020080 s

INFO 2022-06-07 17:23:18,054 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 17:23:21,751 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 17:23:22,275 [__init__.py:postprocess_onnx:77]
class: Chihuahua , confidence:  81.2465 %, index: 151

INFO 2022-06-07 17:23:22,276 [__init__.py:postprocess_onnx:77]
class: miniature pinscher , confidence:  15.0776 %, index: 237

INFO 2022-06-07 17:23:22,278 [__init__.py:postprocess_onnx:77]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 17:23:22,279 [__init__.py:postprocess_onnx:77]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 17:23:22,279 [__init__.py:inference:51]
Time for inference:  0.016845 s

INFO 2022-06-07 17:23:28,158 [inference.py:<module>:19]
>>>>> Pretrain model <<<<<

INFO 2022-06-07 17:23:30,520 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 17:23:30,521 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0777 %, index: 237

INFO 2022-06-07 17:23:30,522 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6574 %, index: 171

INFO 2022-06-07 17:23:30,523 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 17:23:30,524 [inference.py:<module>:30]
Time for inference:  0.040989 s

INFO 2022-06-07 17:23:36,178 [inference.py:<module>:17]
>>>>> ONNX model <<<<<

INFO 2022-06-07 17:23:39,603 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 17:23:40,116 [__init__.py:postprocess_onnx:77]
class: Chihuahua , confidence:  81.2464 %, index: 151

INFO 2022-06-07 17:23:40,117 [__init__.py:postprocess_onnx:77]
class: miniature pinscher , confidence:  15.0777 %, index: 237

INFO 2022-06-07 17:23:40,118 [__init__.py:postprocess_onnx:77]
class: Italian greyhound , confidence:  1.6573 %, index: 171

INFO 2022-06-07 17:23:40,119 [__init__.py:postprocess_onnx:77]
class: toy terrier , confidence:  1.0199 %, index: 158

INFO 2022-06-07 17:23:40,120 [__init__.py:inference:51]
Time for inference:  0.016246 s

INFO 2022-06-07 17:23:50,891 [inference.py:<module>:17]
>>>>> TensorRt engine <<<<<

INFO 2022-06-07 17:23:50,895 [inference.py:<module>:18]
✅ Load tensorrt engine done

INFO 2022-06-07 17:23:50,909 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.5375 %, index: 151

INFO 2022-06-07 17:23:50,911 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 14.7993 %, index: 237

INFO 2022-06-07 17:23:50,912 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6645 %, index: 171

INFO 2022-06-07 17:23:50,913 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0030 %, index: 158

INFO 2022-06-07 17:23:50,914 [inference.py:<module>:25]
Time for inference:  0.014732 s

INFO 2022-06-07 17:27:25,253 [inference.py:<module>:17]
>>>>> Triton server <<<<<

INFO 2022-06-07 17:27:25,256 [inference.py:<module>:22]
✅ Load triton client done

INFO 2022-06-07 17:27:25,943 [utils.py:postprocess:37]
class: Chihuahua , confidence: 81.2464 %, index: 151

INFO 2022-06-07 17:27:25,944 [utils.py:postprocess:37]
class: miniature pinscher , confidence: 15.0776 %, index: 237

INFO 2022-06-07 17:27:25,945 [utils.py:postprocess:37]
class: Italian greyhound , confidence: 1.6573 %, index: 171

INFO 2022-06-07 17:27:25,946 [utils.py:postprocess:37]
class: toy terrier , confidence: 1.0199 %, index: 158

INFO 2022-06-07 17:27:25,947 [inference.py:<module>:32]
Time for inference:  0.015226 s

INFO 2022-06-07 21:48:22,250 [test.py:test_speed:22]

    Batch size: 4 
    Time inferene with 100 loop is 2.1700 s
    >>>> About: 0.021700 s per 1 loop
    

INFO 2022-06-07 21:49:19,037 [test.py:test_speed:22]

    Batch size: 4 
    Time inferene with 100 loop is 2.1272 s
    >>>> About: 0.021272 s per 1 loop
    

INFO 2022-06-07 21:50:38,934 [test.py:test_speed:22]

    Batch size: 4 
    Time inferene with 100 loop is 2.1298 s
    >>>> About: 0.021298 s per 1 loop
    

INFO 2022-06-07 21:53:02,038 [4021296368.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.1076 s
    >>>> About: 0.021076 s per 1 loop
    

INFO 2022-06-07 21:53:33,417 [2443942387.py:test_speed:19]

    Batch size: 1 
    Time inferene with 100 loop is 0.8375 s
    >>>> About: 0.008375 s per 1 loop
    

INFO 2022-06-07 21:54:28,727 [1677535356.py:test_speed:19]

    Batch size: 2 
    Time inferene with 100 loop is 1.2897 s
    >>>> About: 0.012897 s per 1 loop
    

INFO 2022-06-07 21:55:00,128 [4021296368.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.0962 s
    >>>> About: 0.020962 s per 1 loop
    

INFO 2022-06-07 21:55:10,589 [4021296368.py:test_speed:19]

    Batch size: 4 
    Time inferene with 100 loop is 2.1182 s
    >>>> About: 0.021182 s per 1 loop
    

INFO 2022-06-07 21:56:21,246 [578231317.py:test_speed:19]

    Batch size: 8 
    Time inferene with 100 loop is 4.1001 s
    >>>> About: 0.041001 s per 1 loop
    

INFO 2022-06-07 21:57:05,491 [2629808967.py:test_speed:19]

    Batch size: 16 
    Time inferene with 100 loop is 6.9211 s
    >>>> About: 0.069211 s per 1 loop
    

INFO 2022-06-07 21:57:47,436 [2559357562.py:test_speed:19]

    Batch size: 24 
    Time inferene with 100 loop is 10.7489 s
    >>>> About: 0.107489 s per 1 loop
    

INFO 2022-06-07 21:59:42,982 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 21:59:44,835 [2883874147.py:test_speed:18]

    Batch size: 1 
    Time inferene with 100 loop is 1.8459 s
    >>>> About: 0.018459 s per 1 loop
    

INFO 2022-06-07 22:00:20,881 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:00:22,559 [1185592419.py:test_speed:18]

    Batch size: 2 
    Time inferene with 100 loop is 1.6703 s
    >>>> About: 0.016703 s per 1 loop
    

INFO 2022-06-07 22:00:47,550 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:00:49,953 [3258891630.py:test_speed:18]

    Batch size: 4 
    Time inferene with 100 loop is 2.3925 s
    >>>> About: 0.023925 s per 1 loop
    

INFO 2022-06-07 22:01:16,439 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:01:20,314 [2438245524.py:test_speed:18]

    Batch size: 8 
    Time inferene with 100 loop is 3.8599 s
    >>>> About: 0.038599 s per 1 loop
    

INFO 2022-06-07 22:01:55,984 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:02:02,479 [3125992713.py:test_speed:18]

    Batch size: 16 
    Time inferene with 100 loop is 6.4735 s
    >>>> About: 0.064735 s per 1 loop
    

INFO 2022-06-07 22:02:39,236 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:02:48,609 [170935907.py:test_speed:18]

    Batch size: 24 
    Time inferene with 100 loop is 9.3427 s
    >>>> About: 0.093427 s per 1 loop
    

INFO 2022-06-07 22:03:14,997 [__init__.py:load:37]
Load onnx done with cuda

INFO 2022-06-07 22:03:36,904 [346807158.py:test_speed:18]

    Batch size: 32 
    Time inferene with 100 loop is 21.8661 s
    >>>> About: 0.218661 s per 1 loop
    

INFO 2022-06-07 22:05:35,037 [1460358356.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:05:41,277 [1460358356.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:05:41,782 [1460358356.py:<cell line: 8>:19]

    Batch size: 1 
    Time inferene with 100 loop is 0.4960 s
    >>>> About: 0.004960 s per 1 loop
    

INFO 2022-06-07 22:06:17,286 [1527880213.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:06:17,911 [1527880213.py:<cell line: 8>:19]

    Batch size: 2 
    Time inferene with 100 loop is 0.6160 s
    >>>> About: 0.006160 s per 1 loop
    

INFO 2022-06-07 22:06:36,877 [838996783.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:06:37,784 [838996783.py:<cell line: 8>:19]

    Batch size: 4 
    Time inferene with 100 loop is 0.8945 s
    >>>> About: 0.008945 s per 1 loop
    

INFO 2022-06-07 22:06:54,914 [3930734436.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:06:56,615 [3930734436.py:<cell line: 8>:19]

    Batch size: 8 
    Time inferene with 100 loop is 1.6826 s
    >>>> About: 0.016826 s per 1 loop
    

INFO 2022-06-07 22:07:15,392 [426449882.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:07:18,361 [426449882.py:<cell line: 8>:19]

    Batch size: 16 
    Time inferene with 100 loop is 2.9445 s
    >>>> About: 0.029445 s per 1 loop
    

INFO 2022-06-07 22:07:33,733 [1620013645.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:07:38,123 [1620013645.py:<cell line: 8>:19]

    Batch size: 24 
    Time inferene with 100 loop is 4.3545 s
    >>>> About: 0.043545 s per 1 loop
    

INFO 2022-06-07 22:07:55,081 [242108115.py:<cell line: 8>:12]
✅ Load tensorrt engine done

INFO 2022-06-07 22:08:00,844 [242108115.py:<cell line: 8>:19]

    Batch size: 32 
    Time inferene with 100 loop is 5.7186 s
    >>>> About: 0.057186 s per 1 loop
    

INFO 2022-06-07 22:09:40,451 [1069008425.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:09:42,294 [1069008425.py:<cell line: 10>:25]

    Batch size: 1 
    Time inferene with 100 loop is 1.8328 s
    >>>> About: 0.018328 s per 1 loop
    

INFO 2022-06-07 22:10:15,212 [3685875240.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:10:16,665 [3685875240.py:<cell line: 10>:25]

    Batch size: 2 
    Time inferene with 100 loop is 1.4390 s
    >>>> About: 0.014390 s per 1 loop
    

INFO 2022-06-07 22:10:40,287 [1163935773.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:10:42,479 [1163935773.py:<cell line: 10>:25]

    Batch size: 4 
    Time inferene with 100 loop is 2.1832 s
    >>>> About: 0.021832 s per 1 loop
    

INFO 2022-06-07 22:10:59,445 [3930759589.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:11:03,179 [3930759589.py:<cell line: 10>:25]

    Batch size: 8 
    Time inferene with 100 loop is 3.7238 s
    >>>> About: 0.037238 s per 1 loop
    

INFO 2022-06-07 22:11:25,717 [2220311039.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:11:32,591 [2220311039.py:<cell line: 10>:25]

    Batch size: 16 
    Time inferene with 100 loop is 6.8648 s
    >>>> About: 0.068648 s per 1 loop
    

INFO 2022-06-07 22:11:51,474 [3313697619.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:12:01,794 [3313697619.py:<cell line: 10>:25]

    Batch size: 24 
    Time inferene with 100 loop is 10.3113 s
    >>>> About: 0.103113 s per 1 loop
    

INFO 2022-06-07 22:12:17,110 [3864000324.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:12:31,092 [3864000324.py:<cell line: 10>:25]

    Batch size: 32 
    Time inferene with 100 loop is 13.9756 s
    >>>> About: 0.139756 s per 1 loop
    

INFO 2022-06-07 22:13:05,150 [2220311039.py:<cell line: 10>:20]
✅ Load triton client done

INFO 2022-06-07 22:13:12,445 [2220311039.py:<cell line: 10>:25]

    Batch size: 16 
    Time inferene with 100 loop is 7.2864 s
    >>>> About: 0.072864 s per 1 loop
    

